{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":80670,"databundleVersionId":8695879,"sourceType":"competition"},{"sourceId":12165825,"sourceType":"datasetVersion","datasetId":7662283},{"sourceId":12169444,"sourceType":"datasetVersion","datasetId":7664554},{"sourceId":436192,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":355751,"modelId":377051}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW1: Frame-Level Speech Recognition","metadata":{"id":"F9ERgBpbcMmB"}},{"cell_type":"markdown","source":"# In this homework, you will be working with MFCC data consisting of 28 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame.","metadata":{"id":"CLkH6GMGcWcE"}},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"z4vZbDmJvMp1"}},{"cell_type":"code","source":"!pip install torchsummaryX==1.1.0 wandb --quiet","metadata":{"id":"rwYu9sSUnSho","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:00:42.876643Z","iopub.execute_input":"2025-06-15T18:00:42.876848Z","iopub.status.idle":"2025-06-15T18:00:47.330226Z","shell.execute_reply.started":"2025-06-15T18:00:42.876832Z","shell.execute_reply":"2025-06-15T18:00:47.329240Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:00:47.332282Z","iopub.execute_input":"2025-06-15T18:00:47.332590Z","iopub.status.idle":"2025-06-15T18:00:50.353835Z","shell.execute_reply.started":"2025-06-15T18:00:47.332555Z","shell.execute_reply":"2025-06-15T18:00:50.353124Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torchsummaryX import summary\nimport sklearn\nimport gc\nimport zipfile\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport os\nimport datetime\nimport wandb\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qI4qfx7tiBZt","outputId":"ef79a3fc-5689-4e5a-d896-329b8a9d6a5c","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:02:11.031909Z","iopub.execute_input":"2025-06-15T18:02:11.032516Z","iopub.status.idle":"2025-06-15T18:02:18.595529Z","shell.execute_reply.started":"2025-06-15T18:02:11.032479Z","shell.execute_reply":"2025-06-15T18:02:18.594739Z"}},"outputs":[{"name":"stdout","text":"Device:  cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:02:18.596830Z","iopub.execute_input":"2025-06-15T18:02:18.597496Z","iopub.status.idle":"2025-06-15T18:02:18.601594Z","shell.execute_reply.started":"2025-06-15T18:02:18.597474Z","shell.execute_reply":"2025-06-15T18:02:18.600843Z"}},"outputs":[{"name":"stdout","text":"Device:  cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"''' If you are using colab, you can import google drive to save model checkpoints in a folder\n    If you want to use it, uncomment the two lines below\n'''\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"8yBgXjKV1O0Z","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:02:18.602359Z","iopub.execute_input":"2025-06-15T18:02:18.602634Z","iopub.status.idle":"2025-06-15T18:02:18.622262Z","shell.execute_reply.started":"2025-06-15T18:02:18.602605Z","shell.execute_reply":"2025-06-15T18:02:18.621624Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"' If you are using colab, you can import google drive to save model checkpoints in a folder\\n    If you want to use it, uncomment the two lines below\\n'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"### PHONEME LIST\nPHONEMES = [\n            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']","metadata":{"id":"N-9qE20hmCgQ","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:02:18.623561Z","iopub.execute_input":"2025-06-15T18:02:18.623819Z","iopub.status.idle":"2025-06-15T18:02:18.636642Z","shell.execute_reply.started":"2025-06-15T18:02:18.623803Z","shell.execute_reply":"2025-06-15T18:02:18.635969Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Kaggle","metadata":{"id":"ZIi0Big7vPa9"}},{"cell_type":"markdown","source":"This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully.","metadata":{"id":"BBCbeRhixGM7"}},{"cell_type":"code","source":"#!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n#!mkdir /root/.kaggle\n\n#with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n    #f.write('{\"username\":\"Replace this with your Kaggle Username\",\"key\":\"Replace this with your kaggle API key\"}')\n    # Put your kaggle username & key here\n\n#!chmod 600 /root/.kaggle/kaggle.json","metadata":{"id":"TPBUd7Cnl-Rx","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.172564Z","iopub.status.idle":"2025-06-15T05:37:20.172894Z","shell.execute_reply.started":"2025-06-15T05:37:20.172726Z","shell.execute_reply":"2025-06-15T05:37:20.172742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# commands to download data from kaggle\n#!kaggle competitions download -c 11785-hw1p2-f24\n\n#!unzip -qo /content/11785-hw1p2-f24.zip -d '/content'","metadata":{"id":"if2Somqfbje1","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.174722Z","iopub.status.idle":"2025-06-15T05:37:20.175095Z","shell.execute_reply.started":"2025-06-15T05:37:20.174977Z","shell.execute_reply":"2025-06-15T05:37:20.174990Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"Vuzce0_TdcaR"}},{"cell_type":"markdown","source":"This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n\nBefore running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?","metadata":{"id":"2_7QgMbBdgPp"}},{"cell_type":"code","source":"class AudioDataset(torch.utils.data.Dataset):\n\n    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"train-clean-100\"): \n\n        self.context = context\n        self.phonemes = phonemes\n\n        self.mfcc_dir       = os.path.join(root, partition, \"mfcc\")\n        self.transcript_dir = os.path.join(root, partition, \"transcript\")\n\n        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n        transcript_names    = sorted(os.listdir(self.transcript_dir))\n\n        assert len(mfcc_names) == len(transcript_names)\n\n        self.mfccs, self.transcripts = [], []\n\n        for i in range(len(mfcc_names)):\n            mfcc        = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n            mfcc = (mfcc - np.mean(mfcc, axis=0)) / np.std(mfcc, axis=0)\n            transcript  = np.load(os.path.join(self.transcript_dir, transcript_names[i]))\n            \n            if transcript[0] == '[SOS]':\n                transcript = transcript[1:]\n            if transcript[-1] == '[EOS]':\n                transcript = transcript[:-1]\n\n            self.mfccs.append(mfcc)\n            self.transcripts.append(transcript)\n\n        self.mfccs          = np.concatenate(self.mfccs, axis=0)\n        self.transcripts    = np.concatenate([np.array(transcript) for transcript in self.transcripts])\n        self.length = len(self.mfccs)\n        padding = np.zeros((context, self.mfccs.shape[1]))\n        self.mfccs = np.vstack((padding, self.mfccs, padding)) # TODO\n        self.transcripts = np.array([self.phonemes.index(p) for p in self.transcripts])\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, ind):\n        frames = self.mfccs[ind:ind + 2 * self.context + 1]\n        frames = frames.flatten() \n        frames      = torch.FloatTensor(frames)\n        phonemes    = torch.tensor(self.transcripts[ind])\n\n        return frames, phonemes\n","metadata":{"id":"YpLCvi3AJC5z","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:02:27.145978Z","iopub.execute_input":"2025-06-15T18:02:27.146556Z","iopub.status.idle":"2025-06-15T18:02:27.154663Z","shell.execute_reply.started":"2025-06-15T18:02:27.146533Z","shell.execute_reply":"2025-06-15T18:02:27.153974Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class AudioTestDataset(torch.utils.data.Dataset):\n\n    def __init__(self, root, context=0, partition=\"test-clean\"): \n        self.context = context\n        self.mfcc_dir = os.path.join(root, partition, \"mfcc\") \n        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n        self.mfccs = []\n        for i in range(len(mfcc_names)):\n            mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n            mfcc = (mfcc - np.mean(mfcc, axis=0)) / np.std(mfcc, axis=0)\n            self.mfccs.append(mfcc)\n        self.mfccs = np.concatenate(self.mfccs, axis=0)\n        self.length = len(self.mfccs)\n        padding = np.zeros((context, self.mfccs.shape[1]))\n        self.mfccs = np.vstack((padding, self.mfccs, padding))\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, ind):\n        frames = self.mfccs[ind:ind + 2 * self.context + 1]\n        frames = frames.flatten()\n        frames = torch.FloatTensor(frames)\n        return frames\n","metadata":{"id":"e8KfVP39S6o7","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:02:28.719147Z","iopub.execute_input":"2025-06-15T18:02:28.719735Z","iopub.status.idle":"2025-06-15T18:02:28.725738Z","shell.execute_reply.started":"2025-06-15T18:02:28.719711Z","shell.execute_reply":"2025-06-15T18:02:28.725041Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Parameters Configuration","metadata":{"id":"qNacQ8bpt9nw"}},{"cell_type":"markdown","source":"Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments.","metadata":{"id":"WE7tsinAuLNy"}},{"cell_type":"code","source":"config = {\n    'epochs'        : 5,\n    'batch_size'    : 1024,\n    'context'       : 20,\n    'init_lr'       : 1e-3,\n    'architecture'  : 'very-low-cutoff'\n    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n}","metadata":{"id":"PmKwlFqgt_Zq","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.177673Z","iopub.status.idle":"2025-06-15T05:37:20.177946Z","shell.execute_reply.started":"2025-06-15T05:37:20.177825Z","shell.execute_reply":"2025-06-15T05:37:20.177838Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Datasets","metadata":{"id":"2mlwaKlDt_2c"}},{"cell_type":"code","source":"#TODO: Create a dataset object using the AudioDataset class for the training data\nROOT=\"/kaggle/input/11785-hw1p2-f24/11785-f24-hw1p2\"\ntrain_data = AudioDataset(root=ROOT, context=config['context'], phonemes=PHONEMES)\n\n# TODO: Create a dataset object using the AudioDataset class for the validation data\nval_data = AudioDataset(root=ROOT, context=config['context'], phonemes=PHONEMES, partition='dev-clean')\n\n# TODO: Create a dataset object using the AudioTestDataset class for the test data\ntest_data = AudioTestDataset(root=ROOT, context=config['context'], partition='test-clean')","metadata":{"id":"7xi7V8x8W9z4","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.179038Z","iopub.status.idle":"2025-06-15T05:37:20.179254Z","shell.execute_reply.started":"2025-06-15T05:37:20.179151Z","shell.execute_reply":"2025-06-15T05:37:20.179160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.179917Z","iopub.status.idle":"2025-06-15T05:37:20.180118Z","shell.execute_reply.started":"2025-06-15T05:37:20.180024Z","shell.execute_reply":"2025-06-15T05:37:20.180032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcript_dir = '/kaggle/input/11785-hw1p2-f24/11785-f24-hw1p2/dev-clean/transcript'\n\nphoneme_counts = Counter()\n\nfor transcript_file in os.listdir(transcript_dir):\n    if transcript_file.endswith('.npy'):\n        transcript_path = os.path.join(transcript_dir, transcript_file)\n        transcript = np.load(transcript_path, allow_pickle=True)\n        phoneme_counts.update(transcript)\n\n\nleast_common_phoneme = phoneme_counts.most_common()[-1] \nprint(f\"Least common phoneme: {least_common_phoneme[0]} with count {least_common_phoneme[1]}\")\n\ntranscript_dir = '/kaggle/input/11785-hw1p2-f24/11785-f24-hw1p2/dev-clean/transcript'\n\nsil_count = 0\n\nfor transcript_file in os.listdir(transcript_dir):\n    if transcript_file.endswith('.npy'):\n        transcript_path = os.path.join(transcript_dir, transcript_file)\n        transcript = np.load(transcript_path, allow_pickle=True)\n        sil_count += np.sum(transcript == '[SIL]')\n\nprint(f'Total number of \"SIL\" in the dev set: {sil_count}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.181317Z","iopub.status.idle":"2025-06-15T05:37:20.181648Z","shell.execute_reply.started":"2025-06-15T05:37:20.181509Z","shell.execute_reply":"2025-06-15T05:37:20.181525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define dataloaders for train, val and test datasets\n# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n# We shuffle train dataloader but not val & test dataloader. Why?\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset     = train_data,\n    num_workers = 4,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = True\n)\n\nval_loader = torch.utils.data.DataLoader(\n    dataset     = val_data,\n    num_workers = 2,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset     = test_data,\n    num_workers = 2,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\n\nprint(\"Batch size     : \", config['batch_size'])\nprint(\"Context        : \", config['context'])\nprint(\"Input size     : \", (2*config['context']+1)*28)\nprint(\"Output symbols : \", len(PHONEMES))\n\nprint(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\nprint(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\nprint(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n","metadata":{"id":"4mzoYfTKu14s","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.182677Z","iopub.status.idle":"2025-06-15T05:37:20.182966Z","shell.execute_reply.started":"2025-06-15T05:37:20.182791Z","shell.execute_reply":"2025-06-15T05:37:20.182806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing code to check if your data loaders are working\nfor i, data in enumerate(train_loader):\n    frames, phoneme = data\n    print(frames.shape, phoneme.shape)\n    break","metadata":{"id":"n-GV3UvgLSoF","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.184174Z","iopub.status.idle":"2025-06-15T05:37:20.184484Z","shell.execute_reply.started":"2025-06-15T05:37:20.184327Z","shell.execute_reply":"2025-06-15T05:37:20.184341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Network Architecture\n","metadata":{"id":"Nxjwve20JRJ2"}},{"cell_type":"markdown","source":"This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline.","metadata":{"id":"3NJzT-mRw6iy"}},{"cell_type":"code","source":"# This architecture will make you cross the very low cutoff\n# However, you need to run a lot of experiments to cross the medium or high cutoff\nclass Network(torch.nn.Module):\n\n    def __init__(self, input_size, output_size):\n\n        super(Network, self).__init__()\n\n        self.model = torch.nn.Sequential(\n            torch.nn.Linear(input_size, 512),\n            torch.nn.ReLU(),\n            torch.nn.Linear(512, output_size)\n        )\n\n    def forward(self, x):\n        out = self.model(x)\n\n        return out","metadata":{"id":"OvcpontXQq9j","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.185679Z","iopub.status.idle":"2025-06-15T05:37:20.186061Z","shell.execute_reply.started":"2025-06-15T05:37:20.185860Z","shell.execute_reply":"2025-06-15T05:37:20.185878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define Model, Loss Function and Optimizer","metadata":{"id":"HejoSXe3vMVU"}},{"cell_type":"markdown","source":"Here we define the model, loss function, optimizer and optionally a learning rate scheduler.","metadata":{"id":"xAhGBH7-xxth"}},{"cell_type":"code","source":"INPUT_SIZE  = (2*config['context'] + 1) * 28 # Why is this the case?\nmodel       = Network(INPUT_SIZE, len(train_data.phonemes)).to(device)\nsummary(model, frames.to(device))\n# Check number of parameters of your network\n# Remember, you are limited to 20 million parameters for HW1 (including ensembles)","metadata":{"id":"_qtrEM1ZvLje","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.187102Z","iopub.status.idle":"2025-06-15T05:37:20.187399Z","shell.execute_reply.started":"2025-06-15T05:37:20.187246Z","shell.execute_reply":"2025-06-15T05:37:20.187259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n# We use CE because the task is multi-class classification\n\noptimizer = torch.optim.Adam(model.parameters(), lr= config['init_lr']) #Defining Optimizer\n# Recommended : Define Scheduler for Learning Rate,\n# including but not limited to StepLR, MultiStep, CosineAnnealing, CosineAnnealingWithWarmRestarts, ReduceLROnPlateau, etc.\n# You can refer to Pytorch documentation for more information on how to use them.\n#scheduler=torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, )\n# Is your training time very high?\nfrom torch import autocast\nfrom torch.amp import GradScaler\n\n# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\nimport torchaudio.transforms as tat\ntime_mask=tat.TimeMasking(time_mask_param=15)\nfreq_mask=tat.FrequencyMasking(freq_mask_param=15)\n\ndef apply_mask(mfccs):\n    mfccs=time_mask(mfccs)\n    mfccs=freq_mask(mfccs)\n    return mfccs\n# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html","metadata":{"id":"UROGEVJevKD-","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.188401Z","iopub.status.idle":"2025-06-15T05:37:20.188720Z","shell.execute_reply.started":"2025-06-15T05:37:20.188566Z","shell.execute_reply":"2025-06-15T05:37:20.188582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_mask(mfccs):\n    mfccs=time_mask(mfccs)\n    mfccs=freq_mask(mfccs)\n    return mfccs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.189776Z","iopub.status.idle":"2025-06-15T05:37:20.190013Z","shell.execute_reply.started":"2025-06-15T05:37:20.189880Z","shell.execute_reply":"2025-06-15T05:37:20.189889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import autocast\nfrom torch.amp import GradScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.191236Z","iopub.status.idle":"2025-06-15T05:37:20.191583Z","shell.execute_reply.started":"2025-06-15T05:37:20.191377Z","shell.execute_reply":"2025-06-15T05:37:20.191392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training and Validation Functions","metadata":{"id":"IBwunYpyugFg"}},{"cell_type":"markdown","source":"This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs.","metadata":{"id":"1JgeNhx4x2-P"}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"XblOHEVtKab2","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.192578Z","iopub.status.idle":"2025-06-15T05:37:20.192920Z","shell.execute_reply.started":"2025-06-15T05:37:20.192765Z","shell.execute_reply":"2025-06-15T05:37:20.192781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler=GradScaler()\ndef train(model, dataloader, optimizer, criterion):\n\n    model.train()\n    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n\n    for i, (frames, phonemes) in enumerate(dataloader):\n\n        ### Initialize Gradients\n        optimizer.zero_grad()\n\n        ### Move Data to Device (Ideally GPU)\n        frames      = frames.to(device)\n        phonemes    = phonemes.to(device)\n\n        ### Forward Propagation\n        with autocast(device_type='cuda', dtype=torch.float16):\n\n            logits  = model(frames)\n\n            ### Loss Calculation\n            loss    = criterion(logits, phonemes)\n            \n        \n        \n\n        ### Backward Propagation\n        scaler.scale(loss).backward()\n\n        ### Gradient Descent\n        scaler.step(optimizer)\n\n        scaler.update()\n\n        tloss   += loss.item()\n        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n\n        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n        batch_bar.update()\n\n        ### Release memory\n        del frames, phonemes, logits\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n    tloss   /= len(train_loader)\n    tacc    /= len(train_loader)\n\n    return tloss, tacc","metadata":{"id":"8wjPz7DHqKcL","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.193974Z","iopub.status.idle":"2025-06-15T05:37:20.194318Z","shell.execute_reply.started":"2025-06-15T05:37:20.194146Z","shell.execute_reply":"2025-06-15T05:37:20.194160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval(model, dataloader):\n\n    model.eval() # set model in evaluation mode\n    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n\n    for i, (frames, phonemes) in enumerate(dataloader):\n\n        ### Move data to device (ideally GPU)\n        frames      = frames.to(device)\n        phonemes    = phonemes.to(device)\n\n        # makes sure that there are no gradients computed as we are not training the model now\n        with torch.inference_mode():\n            ### Forward Propagation\n            logits  = model(frames)\n            ### Loss Calculation\n            loss    = criterion(logits, phonemes)\n\n        vloss   += loss.item()\n        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n\n        # Do you think we need loss.backward() and optimizer.step() here?\n\n        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n        batch_bar.update()\n\n        ### Release memory\n        del frames, phonemes, logits\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n    vloss   /= len(val_loader)\n    vacc    /= len(val_loader)\n\n    return vloss, vacc","metadata":{"id":"Q5npQNFH315V","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.195336Z","iopub.status.idle":"2025-06-15T05:37:20.195648Z","shell.execute_reply.started":"2025-06-15T05:37:20.195466Z","shell.execute_reply":"2025-06-15T05:37:20.195482Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Weights and Biases Setup","metadata":{"id":"yMd_XxPku5qp"}},{"cell_type":"markdown","source":"This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb.\n\nWe have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning.","metadata":{"id":"tjIbhR1wwbgI"}},{"cell_type":"code","source":"wandb.login(key=\"39e9c89279f6d046c7bae725e099c70ddf0fd98f\") #API Key is in your wandb account, under settings (wandb.ai/settings)","metadata":{"id":"SCDYx5VEu6qI","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.196688Z","iopub.status.idle":"2025-06-15T05:37:20.196930Z","shell.execute_reply.started":"2025-06-15T05:37:20.196830Z","shell.execute_reply":"2025-06-15T05:37:20.196839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create your wandb run\nrun = wandb.init(\n    name    = \"first-run\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n    project = \"hw1p2\", ### Project should be created in your wandb account\n    config  = config ### Wandb Config for your run\n)","metadata":{"id":"xvUnYd3Bw2up","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.198744Z","iopub.status.idle":"2025-06-15T05:37:20.199071Z","shell.execute_reply.started":"2025-06-15T05:37:20.198899Z","shell.execute_reply":"2025-06-15T05:37:20.198913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Save your model architecture as a string with str(model)\nmodel_arch  = str(model)\n\n### Save it in a txt file\narch_file   = open(\"model_arch.txt\", \"w\")\nfile_write  = arch_file.write(model_arch)\narch_file.close()\n\n### log it in your wandb run with wandb.save()\nwandb.save('model_arch.txt')","metadata":{"id":"wft15E_IxYFi","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.200092Z","iopub.status.idle":"2025-06-15T05:37:20.200385Z","shell.execute_reply.started":"2025-06-15T05:37:20.200234Z","shell.execute_reply":"2025-06-15T05:37:20.200248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment","metadata":{"id":"nclx_04fu7Dd"}},{"cell_type":"markdown","source":"Now, it is time to finally run your ablations! Have fun!","metadata":{"id":"MdLMWfEpyGOB"}},{"cell_type":"code","source":"best_val_acc = 0.0\n\nfor epoch in range(config['epochs']):\n\n    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n\n    curr_lr = float(optimizer.param_groups[0]['lr'])\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    val_loss, val_acc = eval(model, val_loader)\n\n    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n\n    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n    \n    if val_acc > best_val_acc:\n        print(\"\\tValidation accuracy improved from {:.04f}% to {:.04f}%! Saving checkpoint...\".format(best_val_acc*100, val_acc*100))\n\n        checkpoint_path = f\"/kaggle/working/checkpoint_epoch_{epoch+1}.pth\"\n        \n        torch.save({\n            'epoch': epoch + 1,             \n            'model_state_dict': model.state_dict(),  \n            'optimizer_state_dict': optimizer.state_dict(),  \n            'loss': val_loss,                \n            'val_acc': val_acc               \n        }, checkpoint_path)\n\n        wandb.save(checkpoint_path)\n\n        best_val_acc = val_acc\n","metadata":{"id":"MG4F77Nm0Am9","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.201938Z","iopub.status.idle":"2025-06-15T05:37:20.202296Z","shell.execute_reply.started":"2025-06-15T05:37:20.202118Z","shell.execute_reply":"2025-06-15T05:37:20.202134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing and submission to Kaggle","metadata":{"id":"_kXwf5YUo_4A"}},{"cell_type":"markdown","source":"Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function.","metadata":{"id":"WI1hSFYLpJvH"}},{"cell_type":"code","source":"checkpoint_path=\"/kaggle/input/jkdsbcjkbdsjbcd/checkpoint_epoch_5.pth\"\ncheckpoint_dict=torch.load(checkpoint_path)\n\nmodel.load_state_dict(checkpoint_dict['model_state_dict'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.203119Z","iopub.status.idle":"2025-06-15T05:37:20.203430Z","shell.execute_reply.started":"2025-06-15T05:37:20.203269Z","shell.execute_reply":"2025-06-15T05:37:20.203283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(model, test_loader):\n    ### What you call for model to perform inference?\n    model.eval() # TODO train or eval?\n\n    ### List to store predicted phonemes of test data\n    test_predictions = []\n\n    ### Which mode do you need to avoid gradients?\n    with torch.no_grad(): # TODO\n\n        for i, mfccs in enumerate(tqdm(test_loader)):\n\n            mfccs   = mfccs.to(device)\n\n            logits  = model(mfccs)\n\n            ### Get most likely predicted phoneme with argmax\n            predicted_phonemes = torch.argmax(logits, dim=1)\n\n            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n            # TODO\n            test_predictions.extend(predicted_phonemes.cpu().numpy())\n    \n\n    return test_predictions","metadata":{"id":"R-SU9fZ3xHtk","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.204591Z","iopub.status.idle":"2025-06-15T05:37:20.204899Z","shell.execute_reply.started":"2025-06-15T05:37:20.204712Z","shell.execute_reply":"2025-06-15T05:37:20.204728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = test(model, test_loader)","metadata":{"id":"wG9v6Xmxu7wp","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.205893Z","iopub.status.idle":"2025-06-15T05:37:20.206197Z","shell.execute_reply.started":"2025-06-15T05:37:20.206044Z","shell.execute_reply":"2025-06-15T05:37:20.206057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def map_id_to_phoneme(predicted_ids):\n    return [PHONEMES[id] for id in predicted_ids]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.207375Z","iopub.status.idle":"2025-06-15T05:37:20.207703Z","shell.execute_reply.started":"2025-06-15T05:37:20.207572Z","shell.execute_reply":"2025-06-15T05:37:20.207587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"phoneme_predictions=map_id_to_phoneme(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.208731Z","iopub.status.idle":"2025-06-15T05:37:20.209011Z","shell.execute_reply.started":"2025-06-15T05:37:20.208840Z","shell.execute_reply":"2025-06-15T05:37:20.208849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Create CSV file with predictions\nwith open(\"./submission1.csv\", \"w+\") as f:\n    f.write(\"id,label\\n\")\n    for i in range(len(phoneme_predictions)):\n        f.write(\"{},{}\\n\".format(i, phoneme_predictions[i]))","metadata":{"id":"ZE1hRnvf0bFz","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.210358Z","iopub.status.idle":"2025-06-15T05:37:20.210614Z","shell.execute_reply.started":"2025-06-15T05:37:20.210500Z","shell.execute_reply":"2025-06-15T05:37:20.210511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Finish your wandb run\nrun.finish()","metadata":{"id":"6Wf-P25TXU0N","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.211159Z","iopub.status.idle":"2025-06-15T05:37:20.211521Z","shell.execute_reply.started":"2025-06-15T05:37:20.211300Z","shell.execute_reply":"2025-06-15T05:37:20.211314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Submit to kaggle competition using kaggle API (Uncomment below to use)\n# !kaggle competitions submit -c 11785-hw1p2-f24 -f ./submission.csv -m \"Test Submission\"\n\n### However, its always safer to download the csv file and then upload to kaggle","metadata":{"id":"LjcammuCxMKN","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.212474Z","iopub.status.idle":"2025-06-15T05:37:20.212742Z","shell.execute_reply.started":"2025-06-15T05:37:20.212617Z","shell.execute_reply":"2025-06-15T05:37:20.212627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n    'epochs'        : 5,\n    'batch_size'    : 256,\n    'context'       : 16,\n    'init_lr'       : 1e-3,\n    'architecture'  : 'pyramid'\n    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.213908Z","iopub.status.idle":"2025-06-15T05:37:20.214202Z","shell.execute_reply.started":"2025-06-15T05:37:20.214075Z","shell.execute_reply":"2025-06-15T05:37:20.214088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Pyramid(torch.nn.Module):\n\n    def __init__(self, input_size, output_size):\n\n        super(Pyramid, self).__init__()\n\n        self.model=torch.nn.Sequential(\n\n            torch.nn.Linear(input_size, max(1024, 10 * input_size)),\n            torch.nn.ReLU(),\n            torch.nn.Linear(max(1024, 10*input_size), 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, output_size)\n            \n        )\n\n        self._init_weights()\n\n\n    def _init_weights(self):\n\n        for m in self.modules():\n            if isinstance(m, torch.nn.Linear):\n                torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n                if m.bias is not None:\n                    torch.nn.init.zeros_(m.bias)\n                    \n    def forward(self, x):\n\n        out=self.model(x)\n\n        return out\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.214779Z","iopub.status.idle":"2025-06-15T05:37:20.214981Z","shell.execute_reply.started":"2025-06-15T05:37:20.214881Z","shell.execute_reply":"2025-06-15T05:37:20.214890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_2=Pyramid(INPUT_SIZE, len(train_data.phonemes)).to(device)\nsummary(model_2, frames.to(device))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.216164Z","iopub.status.idle":"2025-06-15T05:37:20.216477Z","shell.execute_reply.started":"2025-06-15T05:37:20.216331Z","shell.execute_reply":"2025-06-15T05:37:20.216344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer=torch.optim.Adam(model_2.parameters(), lr=config['init_lr'])\n\ncriterion=torch.nn.CrossEntropyLoss()\n\nscheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.217953Z","iopub.status.idle":"2025-06-15T05:37:20.218210Z","shell.execute_reply.started":"2025-06-15T05:37:20.218092Z","shell.execute_reply":"2025-06-15T05:37:20.218105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.219499Z","iopub.status.idle":"2025-06-15T05:37:20.219819Z","shell.execute_reply.started":"2025-06-15T05:37:20.219655Z","shell.execute_reply":"2025-06-15T05:37:20.219670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create your wandb run\nrun = wandb.init(\n    name    = \"second-run\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n    project = \"hw1p2\", ### Project should be created in your wandb account\n    config  = config ### Wandb Config for your run\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.221198Z","iopub.status.idle":"2025-06-15T05:37:20.221576Z","shell.execute_reply.started":"2025-06-15T05:37:20.221358Z","shell.execute_reply":"2025-06-15T05:37:20.221373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Save your model architecture as a string with str(model)\nmodel_arch  = str(model_2)\n\n### Save it in a txt file\narch_file   = open(\"model_arch.txt\", \"w\")\nfile_write  = arch_file.write(model_arch)\narch_file.close()\n\n### log it in your wandb run with wandb.save()\nwandb.save('model_arch.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.222322Z","iopub.status.idle":"2025-06-15T05:37:20.222716Z","shell.execute_reply.started":"2025-06-15T05:37:20.222520Z","shell.execute_reply":"2025-06-15T05:37:20.222536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_val_acc = 0.0\n\nfor epoch in range(config['epochs']):\n\n    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n\n    curr_lr = float(optimizer.param_groups[0]['lr'])\n    train_loss, train_acc = train(model_2, train_loader, optimizer, criterion)\n    val_loss, val_acc = eval(model_2, val_loader)\n    scheduler.step()\n    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n\n    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n    \n    if val_acc > best_val_acc:\n        print(\"\\tValidation accuracy improved from {:.04f}% to {:.04f}%! Saving checkpoint...\".format(best_val_acc*100, val_acc*100))\n\n        checkpoint_path = f\"/kaggle/working/checkpoint_model_2_epoch_{epoch+1}.pth\"\n        \n        torch.save({\n            'epoch': epoch + 1,             \n            'model_state_dict': model_2.state_dict(),  \n            'optimizer_state_dict': optimizer.state_dict(),  \n            'loss': val_loss,                \n            'val_acc': val_acc               \n        }, checkpoint_path)\n\n        wandb.save(checkpoint_path)\n\n        best_val_acc = val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.223719Z","iopub.status.idle":"2025-06-15T05:37:20.223929Z","shell.execute_reply.started":"2025-06-15T05:37:20.223833Z","shell.execute_reply":"2025-06-15T05:37:20.223843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path=\"/kaggle/input/model-2/checkpoint_model_2_epoch_4.pth\"\ncheckpoint_dict=torch.load(checkpoint_path)\n\nmodel_2.load_state_dict(checkpoint_dict['model_state_dict'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.224778Z","iopub.status.idle":"2025-06-15T05:37:20.225239Z","shell.execute_reply.started":"2025-06-15T05:37:20.225061Z","shell.execute_reply":"2025-06-15T05:37:20.225079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_2 = test(model_2, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.226252Z","iopub.status.idle":"2025-06-15T05:37:20.226581Z","shell.execute_reply.started":"2025-06-15T05:37:20.226398Z","shell.execute_reply":"2025-06-15T05:37:20.226414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"phoneme_predictions_2=map_id_to_phoneme(predictions_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.228082Z","iopub.status.idle":"2025-06-15T05:37:20.228318Z","shell.execute_reply.started":"2025-06-15T05:37:20.228214Z","shell.execute_reply":"2025-06-15T05:37:20.228224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Create CSV file with predictions\nwith open(\"./submission2.csv\", \"w+\") as f:\n    f.write(\"id,label\\n\")\n    for i in range(len(phoneme_predictions_2)):\n        f.write(\"{},{}\\n\".format(i, phoneme_predictions_2[i]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:37:20.229387Z","iopub.status.idle":"2025-06-15T05:37:20.230035Z","shell.execute_reply.started":"2025-06-15T05:37:20.229829Z","shell.execute_reply":"2025-06-15T05:37:20.229846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class InvertedPyramid(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super(InvertedPyramid, self).__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Linear(input_size, 2048),\n            torch.nn.BatchNorm1d(2048),\n            torch.nn.GELU(),\n            torch.nn.Dropout(p=0.15),  \n            torch.nn.Linear(2048, 2048),\n            torch.nn.BatchNorm1d(2048),\n            torch.nn.GELU(),\n            torch.nn.Dropout(p=0.15),  \n            torch.nn.Linear(2048, 2048),\n            torch.nn.BatchNorm1d(2048),\n            torch.nn.GELU(),\n            torch.nn.Dropout(p=0.15), \n            torch.nn.Linear(2048, 2048),\n            torch.nn.BatchNorm1d(2048),\n            torch.nn.GELU(),\n            torch.nn.Dropout(p=0.15), \n            torch.nn.Linear(2048, 1024),\n            torch.nn.BatchNorm1d(1024),\n            torch.nn.GELU(),\n            torch.nn.Dropout(p=0.15),  \n            torch.nn.Linear(1024, 1024),\n            torch.nn.BatchNorm1d(1024),\n            torch.nn.GELU(),\n            torch.nn.Linear(1024, 512),\n            torch.nn.BatchNorm1d(512),\n            torch.nn.GELU(),\n            torch.nn.Linear(512, 256),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.GELU(),\n            torch.nn.Linear(256, 128),\n            torch.nn.BatchNorm1d(128),\n            torch.nn.GELU(),\n            torch.nn.Linear(128, output_size)\n        )\n        self._init_weights()\n\n    def _init_weights(self):\n\n        for m in self.modules():\n            if isinstance(m, torch.nn.Linear):\n                torch.nn.init.kaiming_normal_(m.weight)\n\n    \n\n    def forward(self, x):\n        out=self.model(x)\n        return out\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:15:41.011677Z","iopub.execute_input":"2025-06-15T06:15:41.011949Z","iopub.status.idle":"2025-06-15T06:15:41.021194Z","shell.execute_reply.started":"2025-06-15T06:15:41.011928Z","shell.execute_reply":"2025-06-15T06:15:41.020475Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"config = {\n    'epochs'        : 90,\n    'batch_size'    : 4096,\n    'context'       : 25,\n    'init_lr'       : 1e-3,\n    'architecture'  : 'InvertedPyramid'\n    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:15:42.710148Z","iopub.execute_input":"2025-06-15T06:15:42.710423Z","iopub.status.idle":"2025-06-15T06:15:42.715794Z","shell.execute_reply.started":"2025-06-15T06:15:42.710405Z","shell.execute_reply":"2025-06-15T06:15:42.714988Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"#TODO: Create a dataset object using the AudioDataset class for the training data\nROOT=\"/kaggle/input/11785-hw1p2-f24/11785-f24-hw1p2\"\ntrain_data = AudioDataset(root=ROOT, context=config['context'], phonemes=PHONEMES)\n\n# TODO: Create a dataset object using the AudioDataset class for the validation data\nval_data = AudioDataset(root=ROOT, context=config['context'], phonemes=PHONEMES, partition='dev-clean')\n\n# TODO: Create a dataset object using the AudioTestDataset class for the test data\ntest_data = AudioTestDataset(root=ROOT, context=config['context'], partition='test-clean')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:15:46.225145Z","iopub.execute_input":"2025-06-15T06:15:46.225495Z","iopub.status.idle":"2025-06-15T06:23:22.442050Z","shell.execute_reply.started":"2025-06-15T06:15:46.225438Z","shell.execute_reply":"2025-06-15T06:23:22.441252Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"transcript_dir = '/kaggle/input/11785-hw1p2-f24/11785-f24-hw1p2/dev-clean/transcript'\n\nphoneme_counts = Counter()\n\nfor transcript_file in os.listdir(transcript_dir):\n    if transcript_file.endswith('.npy'):\n        transcript_path = os.path.join(transcript_dir, transcript_file)\n        transcript = np.load(transcript_path, allow_pickle=True)\n        phoneme_counts.update(transcript)\n\n\nleast_common_phoneme = phoneme_counts.most_common()[-1] \nprint(f\"Least common phoneme: {least_common_phoneme[0]} with count {least_common_phoneme[1]}\")\n\ntranscript_dir = '/kaggle/input/11785-hw1p2-f24/11785-f24-hw1p2/dev-clean/transcript'\n\nsil_count = 0\n\nfor transcript_file in os.listdir(transcript_dir):\n    if transcript_file.endswith('.npy'):\n        transcript_path = os.path.join(transcript_dir, transcript_file)\n        transcript = np.load(transcript_path, allow_pickle=True)\n        sil_count += np.sum(transcript == '[SIL]')\n\nprint(f'Total number of \"SIL\" in the dev set: {sil_count}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:23:31.749675Z","iopub.execute_input":"2025-06-15T06:23:31.750327Z","iopub.status.idle":"2025-06-15T06:23:37.262061Z","shell.execute_reply.started":"2025-06-15T06:23:31.750303Z","shell.execute_reply":"2025-06-15T06:23:37.261165Z"}},"outputs":[{"name":"stdout","text":"Least common phoneme: ZH with count 869\nTotal number of \"SIL\" in the dev set: 319908\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"# Define dataloaders for train, val and test datasets\n# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n# We shuffle train dataloader but not val & test dataloader. Why?\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset     = train_data,\n    num_workers = 4,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = True\n)\n\nval_loader = torch.utils.data.DataLoader(\n    dataset     = val_data,\n    num_workers = 2,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset     = test_data,\n    num_workers = 2,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\n\nprint(\"Batch size     : \", config['batch_size'])\nprint(\"Context        : \", config['context'])\nprint(\"Input size     : \", (2*config['context']+1)*28)\nprint(\"Output symbols : \", len(PHONEMES))\n\nprint(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\nprint(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\nprint(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:23:46.979761Z","iopub.execute_input":"2025-06-15T06:23:46.980031Z","iopub.status.idle":"2025-06-15T06:23:46.994884Z","shell.execute_reply.started":"2025-06-15T06:23:46.980014Z","shell.execute_reply":"2025-06-15T06:23:46.993970Z"}},"outputs":[{"name":"stdout","text":"Batch size     :  4096\nContext        :  25\nInput size     :  1428\nOutput symbols :  42\nTrain dataset samples = 36091157, batches = 8812\nValidation dataset samples = 1928204, batches = 471\nTest dataset samples = 1934138, batches = 473\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"# Testing code to check if your data loaders are working\nfor i, data in enumerate(train_loader):\n    frames, phoneme = data\n    print(frames.shape, phoneme.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:23:52.169491Z","iopub.execute_input":"2025-06-15T06:23:52.170088Z","iopub.status.idle":"2025-06-15T06:23:58.841575Z","shell.execute_reply.started":"2025-06-15T06:23:52.170059Z","shell.execute_reply":"2025-06-15T06:23:58.840855Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4096, 1428]) torch.Size([4096])\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"INPUT_SIZE  = (2*config['context'] + 1) * 28 # Why is this the case?\nmodel_3      = InvertedPyramid(INPUT_SIZE, len(train_data.phonemes)).to(device)\nsummary(model_3, frames.to(device))\n# Check number of parameters of your network\n# Remember, you are limited to 20 million parameters for HW1 (including ensembles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:24:04.081866Z","iopub.execute_input":"2025-06-15T06:24:04.082181Z","iopub.status.idle":"2025-06-15T06:24:04.440861Z","shell.execute_reply.started":"2025-06-15T06:24:04.082153Z","shell.execute_reply":"2025-06-15T06:24:04.440059Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\nLayer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n====================================================================================================\n0_Linear                [1428, 2048]         [4096, 2048]             2,926.59                 2.92\n1_BatchNorm1d                 [2048]         [4096, 2048]                 4.10                 0.00\n2_GELU                             -         [4096, 2048]                    -                    -\n3_Dropout                          -         [4096, 2048]                    -                    -\n4_Linear                [2048, 2048]         [4096, 2048]             4,196.35                 4.19\n5_BatchNorm1d                 [2048]         [4096, 2048]                 4.10                 0.00\n6_GELU                             -         [4096, 2048]                    -                    -\n7_Dropout                          -         [4096, 2048]                    -                    -\n8_Linear                [2048, 2048]         [4096, 2048]             4,196.35                 4.19\n9_BatchNorm1d                 [2048]         [4096, 2048]                 4.10                 0.00\n10_GELU                            -         [4096, 2048]                    -                    -\n11_Dropout                         -         [4096, 2048]                    -                    -\n12_Linear               [2048, 2048]         [4096, 2048]             4,196.35                 4.19\n13_BatchNorm1d                [2048]         [4096, 2048]                 4.10                 0.00\n14_GELU                            -         [4096, 2048]                    -                    -\n15_Dropout                         -         [4096, 2048]                    -                    -\n16_Linear               [2048, 1024]         [4096, 1024]             2,098.18                 2.10\n17_BatchNorm1d                [1024]         [4096, 1024]                 2.05                 0.00\n18_GELU                            -         [4096, 1024]                    -                    -\n19_Dropout                         -         [4096, 1024]                    -                    -\n20_Linear               [1024, 1024]         [4096, 1024]             1,049.60                 1.05\n21_BatchNorm1d                [1024]         [4096, 1024]                 2.05                 0.00\n22_GELU                            -         [4096, 1024]                    -                    -\n23_Linear                [1024, 512]          [4096, 512]               524.80                 0.52\n24_BatchNorm1d                 [512]          [4096, 512]                 1.02                 0.00\n25_GELU                            -          [4096, 512]                    -                    -\n26_Linear                 [512, 256]          [4096, 256]               131.33                 0.13\n27_BatchNorm1d                 [256]          [4096, 256]                 0.51                 0.00\n28_GELU                            -          [4096, 256]                    -                    -\n29_Linear                 [256, 128]          [4096, 128]                32.90                 0.03\n30_BatchNorm1d                 [128]          [4096, 128]                 0.26                 0.00\n31_GELU                            -          [4096, 128]                    -                    -\n32_Linear                  [128, 42]           [4096, 42]                 5.42                 0.01\n====================================================================================================\n# Params:    19,380.14K\n# Mult-Adds: 19.36M\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss() \n\noptimizer = torch.optim.AdamW(model.parameters(), lr=config['init_lr'], weight_decay=0.01)\n\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 80], gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:24:11.511289Z","iopub.execute_input":"2025-06-15T06:24:11.511828Z","iopub.status.idle":"2025-06-15T06:24:11.517490Z","shell.execute_reply.started":"2025-06-15T06:24:11.511803Z","shell.execute_reply":"2025-06-15T06:24:11.516793Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:24:16.330766Z","iopub.execute_input":"2025-06-15T06:24:16.331278Z","iopub.status.idle":"2025-06-15T06:24:16.653867Z","shell.execute_reply.started":"2025-06-15T06:24:16.331252Z","shell.execute_reply":"2025-06-15T06:24:16.653110Z"}},"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"1550"},"metadata":{}}],"execution_count":117},{"cell_type":"code","source":"wandb.login(key=\"39e9c89279f6d046c7bae725e099c70ddf0fd98f\") #API Key is in your wandb account, under settings (wandb.ai/settings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:24:20.020983Z","iopub.execute_input":"2025-06-15T06:24:20.021484Z","iopub.status.idle":"2025-06-15T06:24:20.029767Z","shell.execute_reply.started":"2025-06-15T06:24:20.021458Z","shell.execute_reply":"2025-06-15T06:24:20.029005Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":118},{"cell_type":"code","source":"# Create your wandb run\nrun = wandb.init(\n    name    = \"third-run\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n    project = \"hw1p2\", ### Project should be created in your wandb account\n    config  = config ### Wandb Config for your run\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:24:20.335425Z","iopub.execute_input":"2025-06-15T06:24:20.335665Z","iopub.status.idle":"2025-06-15T06:24:28.059963Z","shell.execute_reply.started":"2025-06-15T06:24:20.335650Z","shell.execute_reply":"2025-06-15T06:24:28.059388Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">third-run</strong> at: <a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2/runs/a2vg8t8x' target=\"_blank\">https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2/runs/a2vg8t8x</a><br> View project at: <a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2' target=\"_blank\">https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250615_050025-a2vg8t8x/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250615_062420-yd1r1er2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2/runs/yd1r1er2' target=\"_blank\">third-run</a></strong> to <a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2' target=\"_blank\">https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2/runs/yd1r1er2' target=\"_blank\">https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2/runs/yd1r1er2</a>"},"metadata":{}}],"execution_count":119},{"cell_type":"code","source":"### Save your model architecture as a string with str(model)\nmodel_arch  = str(model_3)\n\n### Save it in a txt file\narch_file   = open(\"model_arch.txt\", \"w\")\nfile_write  = arch_file.write(model_arch)\narch_file.close()\n\n### log it in your wandb run with wandb.save()\nwandb.save('model_arch.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:24:28.061185Z","iopub.execute_input":"2025-06-15T06:24:28.061381Z","iopub.status.idle":"2025-06-15T06:24:28.069173Z","shell.execute_reply.started":"2025-06-15T06:24:28.061365Z","shell.execute_reply":"2025-06-15T06:24:28.068488Z"}},"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/wandb/run-20250615_062420-yd1r1er2/files/model_arch.txt']"},"metadata":{}}],"execution_count":120},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:31:09.638089Z","iopub.execute_input":"2025-06-15T06:31:09.638456Z","iopub.status.idle":"2025-06-15T06:31:10.645020Z","shell.execute_reply.started":"2025-06-15T06:31:09.638414Z","shell.execute_reply":"2025-06-15T06:31:10.644363Z"}},"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"734"},"metadata":{}}],"execution_count":124},{"cell_type":"code","source":"\ndef train(model, dataloader, optimizer, criterion):\n\n    model.train()\n    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n\n    for i, (frames, phonemes) in enumerate(dataloader):\n\n        ### Initialize Gradients\n        optimizer.zero_grad()\n\n        ### Move Data to Device (Ideally GPU)\n        frames      = frames.to(device)\n        phonemes    = phonemes.to(device)\n\n        frames = apply_mask(frames)\n\n        ### Forward Propagation\n       \n        logits  = model(frames)\n\n            ### Loss Calculation\n        loss    = criterion(logits, phonemes)\n\n        loss.backward()\n        optimizer.step()\n            \n        \n        \n\n       \n\n        tloss   += loss.item()\n        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n\n        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n        batch_bar.update()\n\n        ### Release memory\n        del frames, phonemes, logits\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n    tloss   /= len(train_loader)\n    tacc    /= len(train_loader)\n\n    return tloss, tacc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:31:22.061652Z","iopub.execute_input":"2025-06-15T06:31:22.062301Z","iopub.status.idle":"2025-06-15T06:31:22.069223Z","shell.execute_reply.started":"2025-06-15T06:31:22.062263Z","shell.execute_reply":"2025-06-15T06:31:22.068319Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"best_val_acc = 0.0\n\nfor epoch in range(config['epochs']):\n\n    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n\n    curr_lr = float(optimizer.param_groups[0]['lr'])\n    train_loss, train_acc = train(model_3, train_loader, optimizer, criterion)\n    val_loss, val_acc = eval(model_3, val_loader)\n    scheduler.step()\n    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n\n    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n    \n    if val_acc > best_val_acc:\n        print(\"\\tValidation accuracy improved from {:.04f}% to {:.04f}%! Saving checkpoint...\".format(best_val_acc*100, val_acc*100))\n\n        checkpoint_path = f\"/kaggle/working/checkpoint_model_3_epoch_{epoch+1}.pth\"\n        \n        torch.save({\n            'epoch': epoch + 1,             \n            'model_state_dict': model_3.state_dict(),  \n            'optimizer_state_dict': optimizer.state_dict(),  \n            'loss': val_loss,                \n            'val_acc': val_acc               \n        }, checkpoint_path)\n\n        wandb.save(checkpoint_path)\n\n        best_val_acc = val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:31:24.622074Z","iopub.execute_input":"2025-06-15T06:31:24.622676Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/90\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/8812 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c1a381304f242b2a78bf23d8343a409"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Model 3- InvertedPyramidNet","metadata":{}},{"cell_type":"code","source":"config = {\n    'epochs'        : 5,\n    'batch_size'    : 2048,\n    'context'       : 16,\n    'init_lr'       : 1e-3,\n    'architecture'  : 'InvertedPyramidNet'\n    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:02:53.474057Z","iopub.execute_input":"2025-06-15T18:02:53.474747Z","iopub.status.idle":"2025-06-15T18:02:53.478293Z","shell.execute_reply.started":"2025-06-15T18:02:53.474725Z","shell.execute_reply":"2025-06-15T18:02:53.477506Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#TODO: Create a dataset object using the AudioDataset class for the training data\nROOT=\"/kaggle/input/11785-hw1p2-f24/11785-f24-hw1p2\"\ntrain_data = AudioDataset(root=ROOT, context=config['context'], phonemes=PHONEMES)\n\n# TODO: Create a dataset object using the AudioDataset class for the validation data\nval_data = AudioDataset(root=ROOT, context=config['context'], phonemes=PHONEMES, partition='dev-clean')\n\n# TODO: Create a dataset object using the AudioTestDataset class for the test data\ntest_data = AudioTestDataset(root=ROOT, context=config['context'], partition='test-clean')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:02:55.199750Z","iopub.execute_input":"2025-06-15T18:02:55.200441Z","iopub.status.idle":"2025-06-15T18:15:33.909797Z","shell.execute_reply.started":"2025-06-15T18:02:55.200416Z","shell.execute_reply":"2025-06-15T18:15:33.909176Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:15:58.378862Z","iopub.execute_input":"2025-06-15T18:15:58.379149Z","iopub.status.idle":"2025-06-15T18:15:58.382906Z","shell.execute_reply.started":"2025-06-15T18:15:58.379126Z","shell.execute_reply":"2025-06-15T18:15:58.382101Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"transcript_dir = '/kaggle/input/11785-hw1p2-f24/11785-f24-hw1p2/dev-clean/transcript'\n\nphoneme_counts = Counter()\n\nfor transcript_file in os.listdir(transcript_dir):\n    if transcript_file.endswith('.npy'):\n        transcript_path = os.path.join(transcript_dir, transcript_file)\n        transcript = np.load(transcript_path, allow_pickle=True)\n        phoneme_counts.update(transcript)\n\n\nleast_common_phoneme = phoneme_counts.most_common()[-1] \nprint(f\"Least common phoneme: {least_common_phoneme[0]} with count {least_common_phoneme[1]}\")\n\ntranscript_dir = '/kaggle/input/11785-hw1p2-f24/11785-f24-hw1p2/dev-clean/transcript'\n\nsil_count = 0\n\nfor transcript_file in os.listdir(transcript_dir):\n    if transcript_file.endswith('.npy'):\n        transcript_path = os.path.join(transcript_dir, transcript_file)\n        transcript = np.load(transcript_path, allow_pickle=True)\n        sil_count += np.sum(transcript == '[SIL]')\n\nprint(f'Total number of \"SIL\" in the dev set: {sil_count}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:16:00.666352Z","iopub.execute_input":"2025-06-15T18:16:00.666933Z","iopub.status.idle":"2025-06-15T18:16:04.928000Z","shell.execute_reply.started":"2025-06-15T18:16:00.666909Z","shell.execute_reply":"2025-06-15T18:16:04.927390Z"}},"outputs":[{"name":"stdout","text":"Least common phoneme: ZH with count 869\nTotal number of \"SIL\" in the dev set: 319908\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Define dataloaders for train, val and test datasets\n# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n# We shuffle train dataloader but not val & test dataloader. Why?\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset     = train_data,\n    num_workers = 4,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = True\n)\n\nval_loader = torch.utils.data.DataLoader(\n    dataset     = val_data,\n    num_workers = 2,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset     = test_data,\n    num_workers = 2,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\n\nprint(\"Batch size     : \", config['batch_size'])\nprint(\"Context        : \", config['context'])\nprint(\"Input size     : \", (2*config['context']+1)*28)\nprint(\"Output symbols : \", len(PHONEMES))\n\nprint(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\nprint(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\nprint(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:16:13.070949Z","iopub.execute_input":"2025-06-15T18:16:13.071230Z","iopub.status.idle":"2025-06-15T18:16:13.078174Z","shell.execute_reply.started":"2025-06-15T18:16:13.071210Z","shell.execute_reply":"2025-06-15T18:16:13.077553Z"}},"outputs":[{"name":"stdout","text":"Batch size     :  2048\nContext        :  16\nInput size     :  924\nOutput symbols :  42\nTrain dataset samples = 36091157, batches = 17623\nValidation dataset samples = 1928204, batches = 942\nTest dataset samples = 1934138, batches = 945\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Testing code to check if your data loaders are working\nfor i, data in enumerate(train_loader):\n    frames, phoneme = data\n    print(frames.shape, phoneme.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:16:17.693897Z","iopub.execute_input":"2025-06-15T18:16:17.694177Z","iopub.status.idle":"2025-06-15T18:16:23.497102Z","shell.execute_reply.started":"2025-06-15T18:16:17.694157Z","shell.execute_reply":"2025-06-15T18:16:23.496194Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2048, 924]) torch.Size([2048])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass InvertedPyramidNet(nn.Module):\n    def __init__(self, input_dim, output_dim, dropout=0.25):\n        super(InvertedPyramidNet, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.BatchNorm1d(input_dim),\n            nn.Linear(input_dim, 2048),\n            nn.BatchNorm1d(2048),\n            nn.Softplus(),\n            nn.Dropout(dropout),\n\n            nn.Linear(2048, 1024),\n            nn.BatchNorm1d(1024),\n            nn.Softplus(),\n            nn.Dropout(dropout),\n\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.Softplus(),\n            nn.Dropout(dropout),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Softplus(),\n            nn.Dropout(dropout),\n\n            nn.Linear(256, output_dim)\n        )\n\n        # Xavier Initialization\n        for m in self.model:\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:16:26.620155Z","iopub.execute_input":"2025-06-15T18:16:26.620527Z","iopub.status.idle":"2025-06-15T18:16:26.627551Z","shell.execute_reply.started":"2025-06-15T18:16:26.620499Z","shell.execute_reply":"2025-06-15T18:16:26.626948Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"INPUT_SIZE  = (2*config['context'] + 1) * 28 # Why is this the case?\nmodel_4      = InvertedPyramidNet(INPUT_SIZE, len(train_data.phonemes)).to(device)\nsummary(model_4, frames.to(device))\n# Check number of parameters of your network\n# Remember, you are limited to 20 million parameters for HW1 (including ensembles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:16:30.044748Z","iopub.execute_input":"2025-06-15T18:16:30.045332Z","iopub.status.idle":"2025-06-15T18:16:30.507420Z","shell.execute_reply.started":"2025-06-15T18:16:30.045309Z","shell.execute_reply":"2025-06-15T18:16:30.506739Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\nLayer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n====================================================================================================\n0_BatchNorm1d                  [924]          [2048, 924]                 1.85                 0.00\n1_Linear                 [924, 2048]         [2048, 2048]             1,894.40                 1.89\n2_BatchNorm1d                 [2048]         [2048, 2048]                 4.10                 0.00\n3_Softplus                         -         [2048, 2048]                    -                    -\n4_Dropout                          -         [2048, 2048]                    -                    -\n5_Linear                [2048, 1024]         [2048, 1024]             2,098.18                 2.10\n6_BatchNorm1d                 [1024]         [2048, 1024]                 2.05                 0.00\n7_Softplus                         -         [2048, 1024]                    -                    -\n8_Dropout                          -         [2048, 1024]                    -                    -\n9_Linear                 [1024, 512]          [2048, 512]               524.80                 0.52\n10_BatchNorm1d                 [512]          [2048, 512]                 1.02                 0.00\n11_Softplus                        -          [2048, 512]                    -                    -\n12_Dropout                         -          [2048, 512]                    -                    -\n13_Linear                 [512, 256]          [2048, 256]               131.33                 0.13\n14_BatchNorm1d                 [256]          [2048, 256]                 0.51                 0.00\n15_Softplus                        -          [2048, 256]                    -                    -\n16_Dropout                         -          [2048, 256]                    -                    -\n17_Linear                  [256, 42]           [2048, 42]                10.79                 0.01\n====================================================================================================\n# Params:    4,669.03K\n# Mult-Adds: 4.66M\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n# We use CE because the task is multi-class classification\n\noptimizer = torch.optim.Adam(model_4.parameters(), lr= config['init_lr']) #Defining Optimizer\n# Recommended : Define Scheduler for Learning Rate,\n# including but not limited to StepLR, MultiStep, CosineAnnealing, CosineAnnealingWithWarmRestarts, ReduceLROnPlateau, etc.\nscheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max')\n# You can refer to Pytorch documentation for more information on how to use them.\n#scheduler=torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, )\n# Is your training time very high?\nfrom torch import autocast\nfrom torch.amp import GradScaler\n\n# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\nimport torchaudio.transforms as tat\ntime_mask=tat.TimeMasking(time_mask_param=15)\nfreq_mask=tat.FrequencyMasking(freq_mask_param=15)\n\ndef apply_mask(mfccs):\n    mfccs=time_mask(mfccs)\n    mfccs=freq_mask(mfccs)\n    return mfccs\n# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:16:38.042998Z","iopub.execute_input":"2025-06-15T18:16:38.043561Z","iopub.status.idle":"2025-06-15T18:16:42.081185Z","shell.execute_reply.started":"2025-06-15T18:16:38.043539Z","shell.execute_reply":"2025-06-15T18:16:42.080599Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:16:44.425136Z","iopub.execute_input":"2025-06-15T18:16:44.425904Z","iopub.status.idle":"2025-06-15T18:16:44.617466Z","shell.execute_reply.started":"2025-06-15T18:16:44.425879Z","shell.execute_reply":"2025-06-15T18:16:44.616919Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"2931"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"scaler=GradScaler()\ndef train(model, dataloader, optimizer, criterion):\n\n    model.train()\n    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n\n    for i, (frames, phonemes) in enumerate(dataloader):\n\n        ### Initialize Gradients\n        optimizer.zero_grad()\n\n        ### Move Data to Device (Ideally GPU)\n        frames      = frames.to(device)\n        phonemes    = phonemes.to(device)\n\n        ### Forward Propagation\n        with autocast(device_type='cuda', dtype=torch.float16):\n\n            logits  = model(frames)\n\n            ### Loss Calculation\n            loss    = criterion(logits, phonemes)\n            \n        \n        \n\n        ### Backward Propagation\n        scaler.scale(loss).backward()\n\n        ### Gradient Descent\n        scaler.step(optimizer)\n\n        scaler.update()\n\n        tloss   += loss.item()\n        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n\n        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n        batch_bar.update()\n\n        ### Release memory\n        del frames, phonemes, logits\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n    tloss   /= len(train_loader)\n    tacc    /= len(train_loader)\n\n    return tloss, tacc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:18:37.448261Z","iopub.execute_input":"2025-06-15T18:18:37.449088Z","iopub.status.idle":"2025-06-15T18:18:37.457813Z","shell.execute_reply.started":"2025-06-15T18:18:37.449040Z","shell.execute_reply":"2025-06-15T18:18:37.457039Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def eval(model, dataloader):\n\n    model.eval() # set model in evaluation mode\n    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n\n    for i, (frames, phonemes) in enumerate(dataloader):\n\n        ### Move data to device (ideally GPU)\n        frames      = frames.to(device)\n        phonemes    = phonemes.to(device)\n\n        # makes sure that there are no gradients computed as we are not training the model now\n        with torch.inference_mode():\n            ### Forward Propagation\n            logits  = model(frames)\n            ### Loss Calculation\n            loss    = criterion(logits, phonemes)\n\n        vloss   += loss.item()\n        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n\n        # Do you think we need loss.backward() and optimizer.step() here?\n\n        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n        batch_bar.update()\n\n        ### Release memory\n        del frames, phonemes, logits\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n    vloss   /= len(val_loader)\n    vacc    /= len(val_loader)\n\n    return vloss, vacc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:16:54.919005Z","iopub.execute_input":"2025-06-15T18:16:54.919299Z","iopub.status.idle":"2025-06-15T18:16:54.925648Z","shell.execute_reply.started":"2025-06-15T18:16:54.919278Z","shell.execute_reply":"2025-06-15T18:16:54.924820Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"wandb.login(key=\"39e9c89279f6d046c7bae725e099c70ddf0fd98f\") #API Key is in your wandb account, under settings (wandb.ai/settings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:16:57.208040Z","iopub.execute_input":"2025-06-15T18:16:57.208611Z","iopub.status.idle":"2025-06-15T18:17:03.877553Z","shell.execute_reply.started":"2025-06-15T18:16:57.208585Z","shell.execute_reply":"2025-06-15T18:17:03.876837Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrishitsaxena55\u001b[0m (\u001b[33mrishitsaxena55-indian-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Create your wandb run\nrun = wandb.init(\n    name    = \"fourth-run\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n    project = \"hw1p2\", ### Project should be created in your wandb account\n    config  = config ### Wandb Config for your run\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:17:07.814268Z","iopub.execute_input":"2025-06-15T18:17:07.815294Z","iopub.status.idle":"2025-06-15T18:17:14.413179Z","shell.execute_reply.started":"2025-06-15T18:17:07.815260Z","shell.execute_reply":"2025-06-15T18:17:14.412577Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250615_181707-mk43jq0n</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2/runs/mk43jq0n' target=\"_blank\">fourth-run</a></strong> to <a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2' target=\"_blank\">https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2/runs/mk43jq0n' target=\"_blank\">https://wandb.ai/rishitsaxena55-indian-institute-of-technology/hw1p2/runs/mk43jq0n</a>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"### Save your model architecture as a string with str(model)\nmodel_arch  = str(model_4)\n\n### Save it in a txt file\narch_file   = open(\"model_arch.txt\", \"w\")\nfile_write  = arch_file.write(model_arch)\narch_file.close()\n\n### log it in your wandb run with wandb.save()\nwandb.save('model_arch.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:17:30.445272Z","iopub.execute_input":"2025-06-15T18:17:30.446020Z","iopub.status.idle":"2025-06-15T18:17:30.454002Z","shell.execute_reply.started":"2025-06-15T18:17:30.445993Z","shell.execute_reply":"2025-06-15T18:17:30.453307Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/wandb/run-20250615_181707-mk43jq0n/files/model_arch.txt']"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"best_val_acc = 0.0\n\nfor epoch in range(config['epochs']):\n\n    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n\n    curr_lr = float(optimizer.param_groups[0]['lr'])\n    train_loss, train_acc = train(model_4, train_loader, optimizer, criterion)\n    val_loss, val_acc = eval(model_4, val_loader)\n    scheduler.step(val_acc)\n    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n\n    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n    \n    if val_acc > best_val_acc:\n        print(\"\\tValidation accuracy improved from {:.04f}% to {:.04f}%! Saving checkpoint...\".format(best_val_acc*100, val_acc*100))\n\n        checkpoint_path = f\"/kaggle/working/checkpoint_model_4_epoch_{epoch+1}.pth\"\n        \n        torch.save({\n            'epoch': epoch + 1,             \n            'model_state_dict': model_4.state_dict(),  \n            'optimizer_state_dict': optimizer.state_dict(),  \n            'loss': val_loss,                \n            'val_acc': val_acc               \n        }, checkpoint_path)\n\n        wandb.save(checkpoint_path)\n\n        best_val_acc = val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:19:00.665307Z","iopub.execute_input":"2025-06-15T18:19:00.665586Z","execution_failed":"2025-06-15T18:34:58.203Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/17623 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8047463e0082469db7e2875195ab7c9a"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass InvertedPyramidNet(nn.Module):\n    def __init__(self, input_dim, output_dim, dropout=0.25):\n        super(InvertedPyramidNet, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.BatchNorm1d(input_dim),\n            nn.Linear(input_dim, 2048),\n            nn.BatchNorm1d(2048),\n            nn.Softplus(),\n            nn.Dropout(dropout),\n\n            nn.Linear(2048, 1024),\n            nn.BatchNorm1d(1024),\n            nn.Softplus(),\n            nn.Dropout(dropout),\n\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.Softplus(),\n            nn.Dropout(dropout),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Softplus(),\n            nn.Dropout(dropout),\n\n            nn.Linear(256, output_dim)\n        )\n\n        # Xavier Initialization\n        for m in self.model:\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T17:59:56.086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = AudioTestDataset(root=ROOT, context=config['context'], partition='test-clean')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T17:59:56.086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loader = torch.utils.data.DataLoader(\n    dataset     = test_data,\n    num_workers = 2,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T17:59:56.086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path=\"/kaggle/input/model_3/pytorch/default/1/checkpoint_model_3_epoch_5.pth\"\ncheckpoint_dict=torch.load(checkpoint_path)\n\nmodel_3.load_state_dict(checkpoint_dict['model_state_dict'])\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T17:59:56.086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(model, test_loader):\n    ### What you call for model to perform inference?\n    model.eval() # TODO train or eval?\n\n    ### List to store predicted phonemes of test data\n    test_predictions = []\n\n    ### Which mode do you need to avoid gradients?\n    with torch.no_grad(): # TODO\n\n        for i, mfccs in enumerate(tqdm(test_loader)):\n\n            mfccs   = mfccs.to(device)\n\n            logits  = model(mfccs)\n\n            ### Get most likely predicted phoneme with argmax\n            predicted_phonemes = torch.argmax(logits, dim=1)\n\n            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n            # TODO\n            test_predictions.extend(predicted_phonemes.cpu().numpy())\n    \n\n    return test_predictions","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T17:59:56.086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_3 = test(model_3, test_loader)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T17:59:56.086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def map_id_to_phoneme(predicted_ids):\n    return [PHONEMES[id] for id in predicted_ids]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T17:59:56.086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"phoneme_predictions_3=map_id_to_phoneme(predictions_3)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T17:59:56.086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Create CSV file with predictions\nwith open(\"./submission3.csv\", \"w+\") as f:\n    f.write(\"id,label\\n\")\n    for i in range(len(phoneme_predictions_3)):\n        f.write(\"{},{}\\n\".format(i, phoneme_predictions_3[i]))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T17:59:56.087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:41:05.225041Z","iopub.execute_input":"2025-06-15T17:41:05.225316Z","iopub.status.idle":"2025-06-15T17:41:05.297373Z","shell.execute_reply.started":"2025-06-15T17:41:05.225295Z","shell.execute_reply":"2025-06-15T17:41:05.296496Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/683076581.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"],"ename":"NameError","evalue":"name 'run' is not defined","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}